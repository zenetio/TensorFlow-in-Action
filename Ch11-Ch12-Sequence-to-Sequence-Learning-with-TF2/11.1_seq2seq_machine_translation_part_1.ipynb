{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models (Sequence-to-Sequence)\n",
    "\n",
    "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11-Ch12-Sequence-to-Sequence-Learning-with-TF2/11.1_seq2seq_machine_translation_part_1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/\n",
    "    \n",
    "german-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# Not setting this led to the following error\n",
    "# _Derived_]RecvAsync is cancelled.   \n",
    "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (Requires manual download)\n",
    "\n",
    "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.1\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Make sure the zip file has been downloaded\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    raise FileNotFoundError(\n",
    "        \"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "Data is in a single `.txt` file. It is a parallel corpus meaning there is a English sentence/phrase/paragraph and a corresponding German translation of it side-by-side. In the file, the source input and the translation are separated by a tab (i.e. tab-seperated file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (255817, 2)\n"
     ]
    }
   ],
   "source": [
    "# Section 11.1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', encoding='utf-8', encoding_errors=\"strict\", header=None)\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are \\xc2\\xa0 (undecode-able bytes remaining in some text)\n",
    "# This can cause errors like UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc2 in position 3: unexpected end of data\n",
    "# when using the TextVectorization layer\n",
    "clean_inds = [i for i in range(len(df)) if b\"\\xc2\" not in df.iloc[i][\"DE\"].encode(\"utf-8\")]\n",
    "\n",
    "df = df.iloc[clean_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255808</th>\n",
       "      <td>Even if some sentences by non-native speakers ...</td>\n",
       "      <td>Auch wenn Sätze von Nichtmuttersprachlern mitu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255809</th>\n",
       "      <td>Remember that the purpose of the Tatoeba Proje...</td>\n",
       "      <td>Es gilt zu bedenken, dass es das Anliegen des ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255810</th>\n",
       "      <td>When I was younger, I hated going to weddings....</td>\n",
       "      <td>Als ich jünger war, hasste ich es, auf Hochzei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255811</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der deine Herkunft nicht kennt, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255812</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       EN  \\\n",
       "255808  Even if some sentences by non-native speakers ...   \n",
       "255809  Remember that the purpose of the Tatoeba Proje...   \n",
       "255810  When I was younger, I hated going to weddings....   \n",
       "255811  If someone who doesn't know your background sa...   \n",
       "255812  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                       DE  \n",
       "255808  Auch wenn Sätze von Nichtmuttersprachlern mitu...  \n",
       "255809  Es gilt zu bedenken, dass es das Anliegen des ...  \n",
       "255810  Als ich jünger war, hasste ich es, auf Hochzei...  \n",
       "255811  Wenn jemand, der deine Herkunft nicht kennt, s...  \n",
       "255812  Wenn jemand Fremdes dir sagt, dass du dich wie...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a smaller sample for computational speed\n",
    "\n",
    "There are more than 220000 samples in the original dataset. We will be using a smaller set of 50000 for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "df = df.sample(n=n_samples, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `SOS` and `EOS` tokens (Decoder)\n",
    "\n",
    "We will add these special tokens to the translated targets. `sos` indicates the start of the sentence and `eos` marks the end of the sentence. \n",
    "\n",
    "E.g. `Grüß Gott!` becomes `sos Grüß Gott! eos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data\n",
    "\n",
    "We will be creating three datasets by sampling randomly (without replacement);\n",
    "\n",
    "* Test dataset - 5000 samples\n",
    "* Validation dataset - 5000 samples\n",
    "* Training dataset - 40000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (5000, 2)\n",
      "valid_df.shape = (5000, 2)\n",
      "train_df.shape = (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary sizes (English and German)\n",
    "\n",
    "Calculate the vocabulary size. We will only consider the words that appear at least 10 times in the corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pgfId-1305275\" href=\"\"></a><span class=\"fm-combinumeral\">#1</span> Create a flattened list from English words.<br>\n",
    "<a id=\"pgfId-1305296\" href=\"\"></a><span class=\"fm-combinumeral\">#2</span> Create a flattened list of German words.<br>\n",
    "<a id=\"pgfId-1305313\" href=\"\"></a><span class=\"fm-combinumeral\">#3</span> Get the vocabulary size of words appearing more than or equal to 10 times.<br>\n",
    "<a id=\"pgfId-1305330\" href=\"\"></a><span class=\"fm-combinumeral\">#4</span> Generate a counter object (i.e., dict word -&gt; frequency).<br>\n",
    "<a id=\"pgfId-1305347\" href=\"\"></a><span class=\"fm-combinumeral\">#5</span> Create a pandas series from the counter, and then sort most frequent to least.<br>\n",
    "<a id=\"pgfId-1305364\" href=\"\"></a><span class=\"fm-combinumeral\">#6</span> Print the most common words.<br>\n",
    "<a id=\"pgfId-1305381\" href=\"\"></a><span class=\"fm-combinumeral\">#7</span> Get the count of words that appear at least 10 times.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9498\n",
      "to     8488\n",
      "I      8243\n",
      "the    6920\n",
      "you    6092\n",
      "a      5800\n",
      "is     4318\n",
      "in     2583\n",
      "of     2544\n",
      "was    2279\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2225\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom       9960\n",
      "Ich       7782\n",
      "ist       4773\n",
      "nicht     4546\n",
      "zu        3528\n",
      "Sie       3374\n",
      "du        3141\n",
      "das       2941\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2482\n"
     ]
    }
   ],
   "source": [
    "# Section 11.1\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create a flattened list from English words\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "# Create a flattened list of German words\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "\n",
    "# Get the vocabulary size of words appearing more than or equal to 10 times\n",
    "n=10\n",
    "\n",
    "# Code listing 11.1\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \n",
    "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
    "    \n",
    "    # Generate a counter object i.e. dict word -> frequency\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    # Create a pandas series from the counter, then sort most frequent to least\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the sequence length (English and German)\n",
    "\n",
    "Here we compute the sequence length of the sequences in the English and German corpora. To ignore the outliers, we only consider data between the 1% and 99% quantiles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pgfId-1305034\" href=\"\"></a><span class=\"fm-combinumeral\">#1</span> Create a pd.Series, which contains the sequence length for each review.<br>\n",
    "<a id=\"pgfId-1305055\" href=\"\"></a><span class=\"fm-combinumeral\">#2</span> Get the median as well as summary statistics of the sequence length.<br>\n",
    "<a id=\"pgfId-1305072\" href=\"\"></a><span class=\"fm-combinumeral\">#3</span> Get the quantiles at given marks (i.e., 1% and 99% percentiles).<br>\n",
    "<a id=\"pgfId-1305089\" href=\"\"></a><span class=\"fm-combinumeral\">#4</span> Print the summary stats of the data between the defined quantiles.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.299175\n",
      "std          2.579978\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         44.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39543.000000\n",
      "mean         6.178995\n",
      "std          2.304810\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          7.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.334825\n",
      "std          2.581789\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         42.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39157.000000\n",
      "mean         8.247746\n",
      "std          2.262261\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%          9.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Section 11.1\n",
    "\n",
    "# Code listing 11.2\n",
    "def print_sequence_length(str_ser):\n",
    "    \n",
    "    \"\"\" Print the summary stats of the sequence length \"\"\"\n",
    "    \n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    # Get the quantiles at given marks\n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    # Print the summary stats of the data between the defined quantlies\n",
    "    print(seq_length_ser[(seq_length_ser >= p_01) & (seq_length_ser < p_99)].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the vocabulary size and sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2225\n",
      "DE vocabulary size: 2482\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow `TextVectorization` layer\n",
    "\n",
    "The `TextVectorization` layer takes in strings and convert them to token IDs. The layer can build a vocabulary using a given text corups and uses that to generate the token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the vectorization layer for English\n",
      "Fitting the EN vectorization layer on data\n",
      "\tDone\n",
      "\n",
      "Defined the vectorization layer for German\n",
      "Fitting the DE vectorization layer on data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "# Section 11.2\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "print(\"Defined the vectorization layer for English\")\n",
    "\n",
    "# Create the text vectorization layer (English)\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "\n",
    "print(\"Fitting the EN vectorization layer on data\")\n",
    "# Here we are calling adapt to fit the vectorization layer with text\n",
    "# so that it learns the vocabulary\n",
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")\n",
    "\n",
    "print(\"\\nDefined the vectorization layer for German\")\n",
    "\n",
    "# Create the text vectorization layer (German)\n",
    "de_vectorize_layer = TextVectorization(\n",
    "    max_tokens=de_vocab,    \n",
    "    output_mode='int',\n",
    "    output_sequence_length=de_seq_length,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "print(\"Fitting the DE vectorization layer on data\")\n",
    "de_vectorize_layer.adapt(np.array(train_df[\"DE\"].tolist()))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TextVectorization` layer in action\n",
    " \n",
    "### How to use the layer (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Input data: \n",
      "[['run'], [\"I'll go home\"], ['ectoplasmic residue']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[421   0   0]\n",
      " [ 74  49 112]\n",
      " [  1   1   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"run\"], [\"I\\'ll go home\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the layer (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 246ms/step\n",
      "Input data: \n",
      "[['[sos] Geh'], ['geh lauf']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[  2 737   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [737   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(de_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"[sos] Geh\"], [\"geh lauf\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of the vocabulary\n",
    "\n",
    "Let's print some words from the two vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "['', '[UNK]', 'tom', 'to', 'you', 'the', 'i', 'a', 'is', 'that']\n",
      "2225\n",
      "\n",
      "German\n",
      "None\n",
      "['', '[UNK]', 'sos', 'eos', 'ich', 'tom', 'nicht', 'ist', 'das', 'du']\n",
      "2482\n"
     ]
    }
   ],
   "source": [
    "# Section 11.2\n",
    "\n",
    "print(\"English\")\n",
    "# Print first few words in the vocabulary\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(en_vectorize_layer.get_vocabulary()))\n",
    "\n",
    "print(\"\\nGerman\")\n",
    "# Print first few words in the vocabulary\n",
    "print(de_vectorize_layer._lookup_layer.input_vocabulary)\n",
    "print(de_vectorize_layer.get_vocabulary()[:10])\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(len(de_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Seq2Seq model\n",
    "\n",
    "Here we define an encoder decoder model to translate between English and German. We will be using a bidirectional encoder and a standard decoder. The model will use Gated Recurrent Unit (GRU) as the recurrent component. The encoder and the decoder has their own `TextVectorization` layers as they use two different languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.2\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Code listing 11.3\n",
    "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    \n",
    "    # Definie an input layer that takes a list of strings (or an array of strings)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # When defining the vocab size, we'd add two for special tokens '' (Padding) and '[UNK]' (Oov tokens)\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,                \n",
    "    )\n",
    "    \n",
    "    # Fit the vectorizer layer on the data\n",
    "    vectorize_layer.adapt(corpus)\n",
    "        \n",
    "    # Get the token IDs\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "        \n",
    "    if not return_vocabulary: \n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name)    \n",
    "    else:\n",
    "        # Returns the vocabulary in addition to the model\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
    "    \n",
    "# Code listing 11.4   \n",
    "def get_encoder(n_vocab, vectorizer):\n",
    "    \"\"\" Define the encoder of the seq2seq model\"\"\"\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='e_embedding')\n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a bidirectional GRU layer\n",
    "    # Encoder looks at the english text (i.e. the input) both backwards and forward\n",
    "    # this leads to better performance\n",
    "    gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, name='e_gru'), name='e_bidirectional_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    # Define the encoder model\n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name='encoder')\n",
    "        \n",
    "    return encoder\n",
    "\n",
    "\n",
    "# Code listing 11.5\n",
    "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
    "    \n",
    "    # Encoder's input\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')    \n",
    "    # Get the encoders final output\n",
    "    d_init_state = encoder(e_inp)\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    # This input layer is used to train the seq2seq model with teacher-forcing\n",
    "    # we feed the German sequence as the input and ask the model to predict \n",
    "    # it with the words offset by 1 (i.e. next word)\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    \n",
    "    # Vectorize the data (assign token IDs)\n",
    "    d_vectorized_out = vectorizer(d_inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    # Note that this is a different embedding layer to the encoder's embedding layer\n",
    "    d_emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='d_embedding')\n",
    "    \n",
    "    # Get the embeddings of the token IDs\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    \n",
    "    # Define a GRU layer\n",
    "    # Unlike the encoder, we cannot define a bidirectional GRU for the decoder\n",
    "    # Why?\n",
    "    d_gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
    "    \n",
    "    # Define an intermediate dense layer\n",
    "    d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
    "    \n",
    "    # The final prediction layer with softmax\n",
    "    d_dense_layer_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax', name='d_dense_final')\n",
    "    d_final_out = d_dense_layer_final(d_dense1_out)\n",
    "    \n",
    "    # Define the full model\n",
    "    seq2seq = tf.keras.models.Model(inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq')\n",
    "    \n",
    "    return seq2seq\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(np.array(train_df[\"EN\"].tolist()), en_vocab, max_length=en_seq_length, name='e_vectorizer')\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(np.array(train_df[\"DE\"].tolist()), de_vocab, max_length=de_seq_length-1, name='d_vectorizer')\n",
    "\n",
    "# Define the final model\n",
    "encoder = get_encoder(en_vocab, en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(de_vocab, encoder, de_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "Compile the model with a suitable loss, an optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"final_seq2seq\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " d_input (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " d_vectorizer (Functional)      (None, 20)           0           ['d_input[0][0]']                \n",
      "                                                                                                  \n",
      " e_input_final (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " d_embedding (Embedding)        (None, 20, 128)      317952      ['d_vectorizer[0][0]']           \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, 256)          483200      ['e_input_final[0][0]']          \n",
      "                                                                                                  \n",
      " d_gru (GRU)                    (None, 20, 256)      296448      ['d_embedding[0][0]',            \n",
      "                                                                  'encoder[0][0]']                \n",
      "                                                                                                  \n",
      " d_dense_1 (Dense)              (None, 20, 512)      131584      ['d_gru[0][0]']                  \n",
      "                                                                                                  \n",
      " d_dense_final (Dense)          (None, 20, 2484)     1274292     ['d_dense_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,503,476\n",
      "Trainable params: 2,503,476\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 11.2\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MT models - BLEU metric\n",
    "\n",
    "In machine translation, a popular choice for assessing performance is the BiLingual Evaluation Understudy (BLEU) metric. Word-to-word accuracy does not reflect the true performance of these models as there can be different ways the same phrase can be translated to. BLEU can take into account such multiple translations when computing the final score. Furthermore, BLEU is superior because it measures precision at multiple n-gram scales between the actual and predicted translations.\n",
    "\n",
    "The implementation is inspired by: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n",
    "\n",
    "### Defining the BLEU metric\n",
    "\n",
    "Below we define a `BLEUMetric` object that can be used to compute the performance of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pgfId-1302173\" href=\"\"></a><span class=\"fm-combinumeral\">#1</span> Get the vocabulary from the fitted TextVectorizer.<br>\n",
    "<a id=\"pgfId-1302201\" href=\"\"></a><span class=\"fm-combinumeral\">#2</span> Define a StringLookup layer, which can convert token IDs to words.<br>\n",
    "<a id=\"pgfId-1302218\" href=\"\"></a><span class=\"fm-combinumeral\">#3</span> Get the predicted token IDs.<br>\n",
    "<a id=\"pgfId-1302238\" href=\"\"></a><span class=\"fm-combinumeral\">#4</span> Convert token IDs to words using the vocabulary and the StringLookup.<br>\n",
    "<a id=\"pgfId-1302255\" href=\"\"></a><span class=\"fm-combinumeral\">#5</span> Strip the string of any extra white spaces.<br>\n",
    "<a id=\"pgfId-1302272\" href=\"\"></a><span class=\"fm-combinumeral\">#6</span> Replace everything after the EOS token with a blank.<br>\n",
    "<a id=\"pgfId-1302289\" href=\"\"></a><span class=\"fm-combinumeral\">#7</span> Join all the tokens to one string in each sequence.<br>\n",
    "<a id=\"pgfId-1302306\" href=\"\"></a><span class=\"fm-combinumeral\">#8</span> Decode the byte stream to a string.<br>\n",
    "<a id=\"pgfId-1302323\" href=\"\"></a><span class=\"fm-combinumeral\">#9</span> If the string is empty, add a [UNK] token. If not, it can lead to numerical errors.<br>\n",
    "<a id=\"pgfId-1302340\" href=\"\"></a><span class=\"fm-combinumeral\">#10</span> Split the sequences into individual tokens.<br>\n",
    "<a id=\"pgfId-1302357\" href=\"\"></a><span class=\"fm-combinumeral\">#11</span> Get the clean versions of the predictions and real sequences.<br>\n",
    "<a id=\"pgfId-1302374\" href=\"\"></a><span class=\"fm-combinumeral\">#12</span> We have to wrap each real sequence in a list to make use of a third-party function to compute BLEU.<br>\n",
    "<a id=\"pgfId-1302391\" href=\"\"></a><span class=\"fm-combinumeral\">#13</span> Get the BLEU value for the given batch of targets and predictions.\n",
    "copy<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.3\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "# Code listing 11.8\n",
    "class BLEUMetric(object):\n",
    "    \n",
    "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
    "      \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n",
    "      super().__init__()\n",
    "      self.vocab = vocabulary\n",
    "      self.id_to_token_layer = StringLookup(vocabulary=self.vocab, num_oov_indices=0, oov_token=\"[KNU]\", invert=True)\n",
    "    \n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  \n",
    "        \n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        def clean_text(tokens):\n",
    "            \n",
    "            \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n",
    "            \n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            translations_in_bytes = tf.strings.strip(\n",
    "                        # 2. Replace everything after the eos token with blank\n",
    "                        tf.strings.regex_replace(\n",
    "                            # 1. Join all the tokens to one string in each sequence\n",
    "                            tf.strings.join(\n",
    "                                tf.transpose(tokens), separator=' '\n",
    "                            ),\n",
    "                        \"eos.*\", \"\"),\n",
    "                   )\n",
    "            \n",
    "            # Decode the byte stream to a string\n",
    "            translations = np.char.decode(\n",
    "                translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n",
    "            )\n",
    "            \n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
    "            \n",
    "            # Split the sequences to individual tokens \n",
    "            translations = np.char.split(translations).tolist()\n",
    "            \n",
    "            return translations\n",
    "        \n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
    "\n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the BLEU metric\n",
    "\n",
    "Below you can see BLEU being used to computer the similarity between a translation (predicted) and reference (true target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with longer correctly predicte phrases: 0.7598356856515925\n",
      "BLEU score without longer correctly predicte phrases: 0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "\n",
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "print(\"BLEU score with longer correctly predicte phrases: {}\".format(bleu1))\n",
    "print(\"BLEU score without longer correctly predicte phrases: {}\".format(bleu2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with a custom loop\n",
    "\n",
    "We will train the model using a custom loop as we want to incorporate BLEU as a metric in our training. We will follow the following procedure;\n",
    "\n",
    "* Each epoch,\n",
    "  * Shuffle the training data\n",
    "  * Train our model on all the training data (in batches)\n",
    "  * Evaluate the model on validation data\n",
    "* Finally, evaluate the model on test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"pgfId-1300335\" href=\"\"></a><span class=\"fm-combinumeral\">#1</span> Define the metric.<br>\n",
    "<a id=\"pgfId-1300356\" href=\"\"></a><span class=\"fm-combinumeral\">#2</span> Define the data.<br>\n",
    "<a id=\"pgfId-1300373\" href=\"\"></a><span class=\"fm-combinumeral\">#3</span> Reset metric logs at the beginning of every epoch.<br>\n",
    "<a id=\"pgfId-1300390\" href=\"\"></a><span class=\"fm-combinumeral\">#4</span> Shuffle data at the beginning of every epoch.<br>\n",
    "<a id=\"pgfId-1300410\" href=\"\"></a><span class=\"fm-combinumeral\">#5</span> Get the number of training batches.<br>\n",
    "<a id=\"pgfId-1300427\" href=\"\"></a><span class=\"fm-combinumeral\">#6</span> Train one batch at a time.<br>\n",
    "<a id=\"pgfId-1300444\" href=\"\"></a><span class=\"fm-combinumeral\">#7</span> Status update<br>\n",
    "<a id=\"pgfId-1300461\" href=\"\"></a><span class=\"fm-combinumeral\">#8</span> Get a batch of inputs (English and German sequences).<br>\n",
    "<a id=\"pgfId-1300478\" href=\"\"></a><span class=\"fm-combinumeral\">#9</span> Get a batch of targets (German sequences offset by 1).<br>\n",
    "<a id=\"pgfId-1300495\" href=\"\"></a><span class=\"fm-combinumeral\">#10</span> Train for a single step.<br>\n",
    "<a id=\"pgfId-1300512\" href=\"\"></a><span class=\"fm-combinumeral\">#11</span> Evaluate the model to get the metrics.<br>\n",
    "<a id=\"pgfId-1300529\" href=\"\"></a><span class=\"fm-combinumeral\">#12</span> Get the final prediction to compute BLEU.<br>\n",
    "<a id=\"pgfId-1300546\" href=\"\"></a><span class=\"fm-combinumeral\">#13</span> Compute the BLEU metric.<br>\n",
    "<a id=\"pgfId-1300563\" href=\"\"></a><span class=\"fm-combinumeral\">#14</span> Update the epoch's log records of the metrics.<br>\n",
    "<a id=\"pgfId-1300580\" href=\"\"></a><span class=\"fm-combinumeral\">#15</span> Define validation data.<br>\n",
    "<a id=\"pgfId-1300597\" href=\"\"></a><span class=\"fm-combinumeral\">#16</span> Evaluate the model on validation data.<br>\n",
    "<a id=\"pgfId-1300614\" href=\"\"></a><span class=\"fm-combinumeral\">#17</span> Print the evaluation metrics of each epoch.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11.3\n",
    "import time\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Code listing 11.6\n",
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n",
    "    \n",
    "    data_dict = {}\n",
    "    for label, df in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist())\n",
    "        de_labels = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist())\n",
    "        data_dict[label] = {'encoder_inputs': en_inputs, 'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Code listing 11.7\n",
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None): \n",
    "    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "        \n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "    \n",
    "    # Return shuffled data\n",
    "    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], de_labels[shuffle_inds]), shuffle_inds\n",
    "\n",
    "\n",
    "# Code listing 11.9\n",
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, batch_size):\n",
    "    \"\"\" Evaluate the model on various metrics such as loss, accuracy and BLEU \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    \n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of batches\n",
    "    n_batches = en_inputs_raw.shape[0]//batch_size\n",
    "    print(\" \", end='\\r')\n",
    "\n",
    "    # Evaluate one batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(\"Evaluating batch {}/{}\".format(i+1, n_batches), end='\\r')\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x, verbose=0)\n",
    "\n",
    "        # Update logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "    \n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)\n",
    "    \n",
    "\n",
    "# Code listing 11.10\n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\" Training the model and evaluating on validation/test sets \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "\n",
    "    shuffle_inds = None\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        bleu_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         Train Phase                                 #\n",
    "        # =================================================================== #\n",
    "\n",
    "        # Shuffle data at the beginning of every epoch\n",
    "        (en_inputs_raw,de_inputs_raw,de_labels_raw), shuffle_inds  = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0]//batch_size\n",
    "\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')\n",
    "\n",
    "            # Get a batch of inputs (english and german sequences)\n",
    "            x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "            # Get a batch of targets (german sequences offset by 1)\n",
    "            y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)        \n",
    "            # Evaluate the model to get the metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            # Get the final prediction to compute BLEU\n",
    "            pred_y = model.predict(x, verbose=0)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                      Validation Phase                               #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        val_en_inputs = data_dict['valid']['encoder_inputs']\n",
    "        val_de_inputs = data_dict['valid']['decoder_inputs']\n",
    "        val_de_labels = data_dict['valid']['decoder_labels']\n",
    "            \n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model, vectorizer, val_en_inputs, val_de_inputs, val_de_labels, batch_size\n",
    "        )\n",
    "            \n",
    "        # Print the evaluation metrics of each epoch\n",
    "        print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
    "        print(\"\\t(train) loss: {} - accuracy: {} - bleu: {}\".format(np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)))\n",
    "        print(\"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(val_loss, val_accuracy, val_bleu))\n",
    "    \n",
    "    # =================================================================== #\n",
    "    #                      Test Phase                                     #\n",
    "    # =================================================================== #    \n",
    "    \n",
    "    test_en_inputs = data_dict['test']['encoder_inputs']\n",
    "    test_de_inputs = data_dict['test']['decoder_inputs']\n",
    "    test_de_labels = data_dict['test']['decoder_labels']\n",
    "            \n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model, vectorizer, test_en_inputs, test_de_inputs, test_de_labels, batch_size\n",
    "    )\n",
    "    \n",
    "    print(\"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(test_loss, test_accuracy, test_bleu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dl_toolkits\\miniconda3\\envs\\manning.tf2\\lib\\site-packages\\numpy\\core\\numeric.py:2463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch 39/39\n",
      "Epoch 1/5\n",
      "\t(train) loss: 1.7550944273288434 - accuracy: 0.24879128696062627 - bleu: 0.002123049560838\n",
      "\t(valid) loss: 1.4457139174143474 - accuracy: 0.332460513481727 - bleu: 0.013267734063560356\n",
      "Evaluating batch 39/39\n",
      "Epoch 2/5\n",
      "\t(train) loss: 1.3106029087152236 - accuracy: 0.37036055078109104 - bleu: 0.02950098674420384\n",
      "\t(valid) loss: 1.2049159453465388 - accuracy: 0.4025515715281169 - bleu: 0.045067310463008284\n",
      "Evaluating batch 39/39\n",
      "Epoch 3/5\n",
      "\t(train) loss: 1.09318800480702 - accuracy: 0.43510486300175005 - bleu: 0.06972906873423713\n",
      "\t(valid) loss: 1.0502231044647021 - accuracy: 0.4519448585999318 - bleu: 0.0828369233741428\n",
      "Evaluating batch 39/39\n",
      "Epoch 4/5\n",
      "\t(train) loss: 0.9339591274276758 - accuracy: 0.4842957123540915 - bleu: 0.10476757940391189\n",
      "\t(valid) loss: 0.9576318584955655 - accuracy: 0.4828882202123984 - bleu: 0.10944701101094335\n",
      "Evaluating batch 39/39\n",
      "Epoch 5/5\n",
      "\t(train) loss: 0.8074664710423886 - accuracy: 0.5278435642711627 - bleu: 0.14234866198511817\n",
      "\t(valid) loss: 0.8760985655662341 - accuracy: 0.5141335870975103 - bleu: 0.12834800455954107\n",
      "Evaluating batch 39/39\n",
      "(test) loss: 0.8718226582576067 - accuracy: 0.5181157054045261 - bleu: 0.13591764881851973\n",
      "\n",
      "It took 948.8964259624481 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()    \n",
    "train_model(final_model, de_vectorizer, train_df, valid_df, test_df, epochs, batch_size)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\nIt took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model\n",
    "\n",
    "We save the trained model as well as the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\seq2seq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\seq2seq\\assets\n"
     ]
    }
   ],
   "source": [
    "# Section 11.3\n",
    "\n",
    "## Save the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(final_model, os.path.join('models', 'seq2seq'))\n",
    "\n",
    "import json\n",
    "os.makedirs(os.path.join('models', 'seq2seq_vocab'), exist_ok=True)\n",
    "\n",
    "# Save the vocabulary files\n",
    "with open(os.path.join('models', 'seq2seq_vocab', 'en_vocab.json'), 'w') as f:\n",
    "    json.dump(en_vocabulary, f)    \n",
    "with open(os.path.join('models', 'seq2seq_vocab', 'de_vocab.json'), 'w') as f:\n",
    "    json.dump(de_vocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model\n",
    "\n",
    "For inference we have to create a new model using the weights of the trained model. During training we used teacher forcing, i.e. providing words from the translation as inputs to the decoder. This cannot be done during inference as we do not have a translation, but want to generate one.\n",
    "\n",
    "Therefore, we create a decoder model that can generate one prediction at a time. We start the prediction process by giving the `sos` token as the initial input to the decoder and keep generating words until the decoder outputs `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabularies\n",
      "Loading weights and generating the inference model\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "# Section 11.4\n",
    "\n",
    "# Code listing 11.11\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_inference_model(save_path):\n",
    "    \"\"\" Load the saved model and create an inference model from that \"\"\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(save_path)\n",
    "    \n",
    "    # Get the encoder model\n",
    "    en_model = model.get_layer(\"encoder\")\n",
    "    \n",
    "    # Define two inputs\n",
    "    # 1. Takes a single word as the input to the decoder\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_infer_input')\n",
    "    # 2. Takes an initial state to pass to the decoder GRU as an input\n",
    "    d_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state')\n",
    "    \n",
    "    # Generate the vectorized output of inp\n",
    "    d_vectorizer = model.get_layer('d_vectorizer')    \n",
    "    d_vectorized_out = d_vectorizer(d_inp)\n",
    "    \n",
    "    # Generate the embeddings from the vectorized input\n",
    "    d_emb_out = model.get_layer('d_embedding')(d_vectorized_out)\n",
    "    \n",
    "    # Get the GRU layer\n",
    "    d_gru_layer = model.get_layer(\"d_gru\")\n",
    "    # Since we generate one word at a time, we will not need the return_sequences\n",
    "    d_gru_layer.return_sequences = False\n",
    "    # Get the GRU out while using d_state_inp from earlier, as the initial state\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp) \n",
    "    \n",
    "    # Get the dense output\n",
    "    d_dense1_out = model.get_layer(\"d_dense_1\")(d_gru_out) \n",
    "    \n",
    "    # Get the final output\n",
    "    d_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out) \n",
    "    \n",
    "    # Define the final decoder\n",
    "    de_model = tf.keras.models.Model(inputs=[d_inp, d_state_inp], outputs=[d_final_out, d_gru_out])\n",
    "    \n",
    "    return en_model, de_model\n",
    "\n",
    "def get_vocabularies(save_dir):\n",
    "    \"\"\" Load the vocabulary files from a given path\"\"\"\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'en_vocab.json'), 'r') as f:\n",
    "        en_vocabulary = json.load(f)\n",
    "        \n",
    "    with open(os.path.join(save_dir, 'de_vocab.json'), 'r') as f:\n",
    "        de_vocabulary = json.load(f)\n",
    "        \n",
    "    return en_vocabulary, de_vocabulary\n",
    "\n",
    "print(\"Loading vocabularies\")\n",
    "en_vocabulary, de_vocabulary = get_vocabularies(os.path.join('models', 'seq2seq_vocab'))\n",
    "\n",
    "print(\"Loading weights and generating the inference model\")\n",
    "en_model, de_model = get_inference_model(os.path.join('models', 'seq2seq'))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new translations\n",
    "\n",
    "Here we generate a new translation by first starting with the `sos` token and asking the decoder to generate words until it outputs `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: My hair is naturally curly.\n",
      "Translation: mein [UNK] hat [UNK] [UNK] eos\n",
      "\n",
      "Input: I think about it every day.\n",
      "Translation: ich denke es [UNK] jeden tag eos\n",
      "\n",
      "Input: I'll never doubt you again.\n",
      "Translation: ich werde nie wieder wieder [UNK] eos\n",
      "\n",
      "Input: Did the doctors give you anything for the pain?\n",
      "Translation: hat die geschichte für ihre [UNK] [UNK] eos\n",
      "\n",
      "Input: She runs fastest in our class.\n",
      "Translation: sie [UNK] in der stadt eos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code listing 11.12\n",
    "def generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text):\n",
    "    \"\"\" Generate a new translation \"\"\"\n",
    "    \n",
    "    start_token = 'sos'\n",
    "    \n",
    "    # Print the input\n",
    "    print(\"Input: {}\".format(sample_en_text))\n",
    "    \n",
    "    # Get the initial state for the decoder\n",
    "    d_state = en_model.predict(np.array([sample_en_text]), verbose=0)\n",
    "    # First word will be sos\n",
    "    de_word = start_token\n",
    "    # We collect the translation in this list\n",
    "    de_translation = []\n",
    "    \n",
    "    # Keep predicting until we get eos\n",
    "    while de_word != 'eos':\n",
    "        # Override the previous state input with the new state\n",
    "        de_pred, d_state = de_model.predict([np.array([de_word]), d_state], verbose=0)    \n",
    "        # Get the actual word from the token ID of the prediction\n",
    "        de_word = de_vocabulary[np.argmax(de_pred[0])]\n",
    "        # Add that to the translation\n",
    "        de_translation.append(de_word)\n",
    "\n",
    "    print(\"Translation: {}\\n\".format(' '.join(de_translation)))\n",
    "\n",
    "for i in range(5):\n",
    "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
    "    generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manning.tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a5eda5f6f277a35ee74e53c7ea4300c8d4d4a1e37dd21ebec0e7d7b2134f5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
