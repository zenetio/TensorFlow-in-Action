{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification with BERT\n",
    "\n",
    "Deep learning has been revolutionized by transformer models. Transformer based models like BERT are heavily used in NLP to solve tasks due to the rich numerical representations of text they provide. Here we will be discussing how to download a transformer model and then adapt it to solve a spam classification problem.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch13-Transormers-with-TF2-and-Huggingface/13.1_Spam_Classification_with_BERT.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.9.3 installed\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_models as tfm\n",
    "\n",
    "import time \n",
    "\n",
    "print(\"TensorFlow: {} installed\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data\n",
    "\n",
    "For this, we will be using the spam classification dataset available [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip). It is a tab separated file with two columns. First column is a single word (ham/spam), where the second column contains the SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zip file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "# Downloading the data\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "        \n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'smsspamcollection.zip')):\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'smsspamcollection.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'SMSSpamCollection')):\n",
    "    with zipfile.ZipFile(os.path.join('data', 'smsspamcollection.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pgfId-1178245\" href=\"\"></a><span class=\"fm-combinumeral\">#1</span> Inputs (messages) are stored here.<br>\n",
    "<a id=\"pgfId-1178266\" href=\"\"></a><span class=\"fm-combinumeral\">#2</span> The label (0/1) is stored here.<br>\n",
    "<a id=\"pgfId-1178283\" href=\"\"></a><span class=\"fm-combinumeral\">#3</span> Counts the total number of ham/spam examples<br>\n",
    "<a id=\"pgfId-1178303\" href=\"\"></a><span class=\"fm-combinumeral\">#4</span> Read every row in the file.<br>\n",
    "<a id=\"pgfId-1178320\" href=\"\"></a><span class=\"fm-combinumeral\">#5</span> If the line starts with ham, it’s a ham example.<br>\n",
    "<a id=\"pgfId-1178337\" href=\"\"></a><span class=\"fm-combinumeral\">#6</span> Assign it a label of 0.<br>\n",
    "<a id=\"pgfId-1178354\" href=\"\"></a><span class=\"fm-combinumeral\">#7</span> Input is the text in the line (except for the word starting ham).<br>\n",
    "<a id=\"pgfId-1178371\" href=\"\"></a><span class=\"fm-combinumeral\">#8</span> Increase the count n_ham.<br>\n",
    "<a id=\"pgfId-1178388\" href=\"\"></a><span class=\"fm-combinumeral\">#9</span> If the line starts with spam, it’s spam.<br>\n",
    "<a id=\"pgfId-1178405\" href=\"\"></a><span class=\"fm-combinumeral\">#10</span> Assign it a label of 1.<br>\n",
    "<a id=\"pgfId-1178422\" href=\"\"></a><span class=\"fm-combinumeral\">#11</span> Input is the text in the line (except for the word that starts spam).<br>\n",
    "<a id=\"pgfId-1178439\" href=\"\"></a><span class=\"fm-combinumeral\">#12</span> Append the input text to inputs.<br>\n",
    "<a id=\"pgfId-1178456\" href=\"\"></a><span class=\"fm-combinumeral\">#13</span> Append the labels to labels.<br>\n",
    "<a id=\"pgfId-1178473\" href=\"\"></a><span class=\"fm-combinumeral\">#14</span> Convert inputs to a NumPy array (and reshape it to a matrix with one column).<br>\n",
    "<a class=\"calibre7\" id=\"pgfId-1178490\" href=\"\"></a><span class=\"fm-combinumeral\">#15</span> Convert the labels list to a NumPy array.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4827 ham and 747 spam\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'Ok lar... Joking wif u oni...\\n', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", 'U dun say so early hor... U c already then say...\\n', \"Nah I don't think he goes to usf, he lives around here though\\n\"]\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.1\n",
    "import numpy as np\n",
    "\n",
    "# Inputs and labels will be stored in this\n",
    "inputs = []\n",
    "labels = []\n",
    "# Total number of instances for spam and ham\n",
    "n_ham, n_spam = 0,0\n",
    "with open(os.path.join('data', 'SMSSpamCollection'), 'r') as f:\n",
    "    for r in f:        \n",
    "        # Ham input\n",
    "        if r.startswith('ham'):\n",
    "            label = 0\n",
    "            txt = r[4:]\n",
    "            n_ham += 1\n",
    "        # Spam input\n",
    "        elif r.startswith('spam'):\n",
    "            label = 1\n",
    "            txt = r[5:]\n",
    "            n_spam += 1\n",
    "        inputs.append(txt)\n",
    "        labels.append(label)\n",
    "        \n",
    "print(\"Found {} ham and {} spam\".format(n_ham, n_spam))\n",
    "print(inputs[:5])\n",
    "\n",
    "# Convert them to arrays\n",
    "inputs = np.array(inputs).reshape(-1,1)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting train/valid/test\n",
    "\n",
    "Here we will split the data to three sets using `imbalanced-learn` library. Specifically we,\n",
    "\n",
    "* Create a balanced test set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced validation set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced train set from the left over instances (undersampled using Near miss algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Valid statistics\n",
      "0    100\n",
      "1    100\n",
      "dtype: int64\n",
      "Train statistics\n",
      "0    4627\n",
      "1     547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "from imblearn.under_sampling import  NearMiss, RandomUnderSampler\n",
    "\n",
    "\n",
    "n=100 # Number of instances for each class for train/validation sets\n",
    "rus = RandomUnderSampler(sampling_strategy={0:n, 1:n}, random_state=random_seed)\n",
    "rus.fit_resample(inputs, labels)\n",
    "\n",
    "# Get test indices\n",
    "test_inds = rus.sample_indices_\n",
    "test_x, test_y = inputs[test_inds], np.array(labels)[test_inds]\n",
    "print(\"Test statistics\")\n",
    "print(pd.Series(test_y).value_counts())\n",
    "\n",
    "# Get rest (train + valid)\n",
    "rest_inds = [i for i in range(inputs.shape[0]) if i not in test_inds]\n",
    "rest_x, rest_y = inputs[rest_inds], labels[rest_inds]\n",
    "\n",
    "# Get valid indices\n",
    "rus.fit_resample(rest_x, rest_y)\n",
    "valid_inds = rus.sample_indices_\n",
    "valid_x, valid_y = rest_x[valid_inds], rest_y[valid_inds]\n",
    "print(\"Valid statistics\")\n",
    "print(pd.Series(valid_y).value_counts())\n",
    "\n",
    "# Rest goes in training\n",
    "train_inds = [i for i in range(rest_x.shape[0]) if i not in valid_inds]\n",
    "train_x, train_y = rest_x[train_inds], rest_y[train_inds]\n",
    "print(\"Train statistics\")\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating the class imbalance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    547\n",
      "1    547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To use near miss algorithm, we need a numerical representation of the messages\n",
    "# We will use the bag of words representation\n",
    "countvec = CountVectorizer()\n",
    "train_bow = countvec.fit_transform(train_x.reshape(-1).tolist())\n",
    "\n",
    "# NearMiss is a common undersampling technique\n",
    "oss = NearMiss()\n",
    "X_res, y_res = oss.fit_resample(train_bow, train_y)\n",
    "train_inds = oss.sample_indices_\n",
    "\n",
    "train_x, train_y = train_x[train_inds], train_y[train_inds]\n",
    "\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is obrained from the articates found in https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\n",
    "\n",
    "# Get the vocab file path from the BERT layer\n",
    "\n",
    "# You can automatically obtain these via the following code. But for ease of understanding I have fixed them to constants\n",
    "# bert_layer = hub.KerasLayer(hub_bert_url, trainable=True)\n",
    "# bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "# do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "vocab_file = os.path.join(\"data\", \"vocab.txt\")\n",
    "do_lower_case = True\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = tfm.nlp.layers.FastWordpieceBertTokenizer(vocab_file=vocab_file, lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding tokenization in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['She sells seashells by the seashore']\n",
      "Tokens IDs generated by BERT: [ 2016 15187 11915 18223  2015  2011  1996 11915 16892]\n",
      "Tokens generated by BERT: ['she', 'sells', 'seas', '##hell', '##s', 'by', 'the', 'seas', '##hore']\n"
     ]
    }
   ],
   "source": [
    "text = [\"She sells seashells by the seashore\"]\n",
    "print(\"Original text: {}\".format(text))\n",
    "#tokens = tokenizer.tokenize(text)\n",
    "tokens = tf.reshape(tokenizer(text), [-1])\n",
    "print(\"Tokens IDs generated by BERT: {}\".format(tokens))\n",
    "ids = [tokenizer._vocab[tid] for tid in tokens] \n",
    "#tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens generated by BERT: {}\".format(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] has ID: 101\n",
      "Token: [SEP] has ID: 102\n",
      "Token: [MASK] has ID: 103\n",
      "Token: [PAD] has ID: 0\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]', '[MASK]', '[PAD]']\n",
    "ids = [tokenizer._vocab.index(tok) for tok in special_tokens]\n",
    "for t, i in zip(special_tokens, ids):\n",
    "    print(\"Token: {} has ID: {}\".format(t, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean       36.256856\n",
       "std        19.915669\n",
       "min        12.000000\n",
       "25%        18.000000\n",
       "50%        25.000000\n",
       "75%        56.000000\n",
       "90%        63.000000\n",
       "max        79.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13.2\n",
    "# Code listing 13.2\n",
    "def encode_sentence(s):\n",
    "    \"\"\" Encode a given sentence by tokenizing it and adding special tokens \"\"\"\n",
    "    \n",
    "    tokens = list(tf.reshape(tokenizer([\"CLS\" + s + \"[SEP]\"]), [-1]))\n",
    "    return tokens\n",
    "\n",
    "seq_lengths = pd.Series([len(encode_sentence(str(s))) for s in train_x])\n",
    "seq_lengths.describe(percentiles=[0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the correct input format for BERT\n",
    "\n",
    "BERT model needs three inputs. These are formed into a dictionary having the following keys.\n",
    "\n",
    "* `input_word_ids` - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* `input_mask` - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* `input_type_ids` - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)\n",
    "\n",
    "`tf-models` library provides a convenient class to pack a given sentence (or a list of sentences) to this format called `BertPackInputs`. We'll use this to create our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_word_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[  101,  2016, 15187, 11915, 18223,  2015,  2011,  1996, 11915,\n",
      "        16892,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]])>, 'input_mask': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'input_type_ids': <tf.Tensor: shape=(1, 60), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 60\n",
    "packer = tfm.nlp.layers.BertPackInputs(\n",
    "    seq_length=max_seq_length,\n",
    "    special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    ")\n",
    "\n",
    "text = [\"She sells seashells by the seashore\"]\n",
    "\n",
    "tok1 = tokenizer(text)\n",
    "\n",
    "packed = packer(tok1)\n",
    "\n",
    "print(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1094 training samples\n",
      "Added 200 validation samples\n",
      "Added 200 test samples\n"
     ]
    }
   ],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Code listing 13.3\n",
    "def get_bert_inputs(tokenizer, docs,max_seq_len=None):\n",
    "    \"\"\" Generate inputs for BERT using a set of documents \"\"\"\n",
    "    \n",
    "    packer = tfm.nlp.layers.BertPackInputs(\n",
    "        seq_length=max_seq_length,\n",
    "        special_tokens_dict = tokenizer.get_special_tokens_dict()\n",
    "    )\n",
    "    \n",
    "    packed = packer(tokenizer(docs))\n",
    "    \n",
    "    packed_numpy = dict([(k, v.numpy()) for k,v in packed.items()])\n",
    "    # Final output\n",
    "    return packed_numpy\n",
    "\n",
    "# Creating train/validation/test data\n",
    "train_inputs = get_bert_inputs(tokenizer, train_x, max_seq_len=60)\n",
    "print(f\"Added {len(train_inputs['input_word_ids'])} training samples\")\n",
    "\n",
    "valid_inputs = get_bert_inputs(tokenizer, valid_x, max_seq_len=60)\n",
    "print(f\"Added {len(valid_inputs['input_word_ids'])} validation samples\")\n",
    "\n",
    "test_inputs = get_bert_inputs(tokenizer, test_x, max_seq_len=60)\n",
    "print(f\"Added {len(test_inputs['input_word_ids'])} test samples\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled train_y labels: [1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Shuffling training data as a precaution\n",
    "train_inds = np.random.permutation(len(train_inputs[\"input_word_ids\"]))\n",
    "train_inputs = dict([(k, v[train_inds]) for k, v in train_inputs.items()])\n",
    "train_y = train_y[train_inds]\n",
    "\n",
    "print(f\"Shuffled train_y labels: {train_y[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the BERT model\n",
    "\n",
    "Here we download the BERT model from the TensorFlow hub. and create a Keras layer from that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the BERT encoder and inputs\n",
    "\n",
    "BERT model needs three inputs,\n",
    "\n",
    "* Input word IDs - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* Input mask - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* Segment IDs - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 768)\n",
      "(None, 60, 768)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n",
    "max_seq_length = 60  # Your choice here.\n",
    "\n",
    "# Input layers\n",
    "\n",
    "# Contains input token ids\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "# Contains input mask values\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "# Contains input type (whether token belongs to sequence A or B) values\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"input_type_ids\")\n",
    "\n",
    "# BERT encoder downloaded from TF hub\n",
    "bert_layer = hub.KerasLayer(hub_bert_url, trainable=True)\n",
    "\n",
    "# get the output of the encoder\n",
    "output = bert_layer({\"input_word_ids\":input_word_ids, \"input_mask\": input_mask, \n",
    "                                             \"input_type_ids\": input_type_ids})\n",
    "\n",
    "# Define the final encoder as with the Functional API\n",
    "hub_encoder = tf.keras.models.Model(\n",
    "    inputs={\"input_word_ids\": input_word_ids, \"input_mask\": input_mask, \"input_type_ids\": input_type_ids}, \n",
    "    outputs={\"sequence_output\": output[\"sequence_output\"], \"pooled_output\": output[\"pooled_output\"]}\n",
    ")\n",
    "\n",
    "# Check the outputs of the Bert layer\n",
    "print(output[\"pooled_output\"].shape)\n",
    "print(output[\"sequence_output\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a downstream classifier from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Section 13.2\n",
    "\n",
    "# Generating a classifier and the encoder\n",
    "bert_classifier = tfm.nlp.models.BertClassifier(network=hub_encoder, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Another alternative way to get the BERT encoder (not pretrained)\n",
    "\n",
    "import yaml\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/configs/models/bert_en_uncased_base.yaml\n",
    "with open(os.path.join(\"data\", \"bert_en_uncased_base.yaml\"), 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)['task']['model']['encoder']['bert']\n",
    "\n",
    "\n",
    "# Print BERT config\n",
    "print(\"BERT Config\")\n",
    "print(config_dict)\n",
    "\n",
    "encoder_config = tfm.nlp.encoders.EncoderConfig({\n",
    "    'type':'bert',\n",
    "    'bert': config_dict\n",
    "})\n",
    "\n",
    "bert_encoder = tfm.nlp.encoders.build_encoder(encoder_config)\n",
    "\n",
    "# Generating a classifier and the encoder\n",
    "bert_classifier = tfm.nlp.models.BertClassifier(\n",
    "    network=bert_encoder, num_classes=2\n",
    ")\n",
    "\"\"\"\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code listing 13.4\n",
    "# Set up epochs and steps\n",
    "epochs = 3\n",
    "batch_size = 56\n",
    "eval_batch_size = 56\n",
    "\n",
    "train_data_size = train_x.shape[0]\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(num_train_steps * 0.1)\n",
    "\n",
    "init_lr = 3e-6\n",
    "end_lr = 0.0\n",
    "\n",
    "# Define the decay\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=init_lr,\n",
    "    end_learning_rate=end_lr,\n",
    "    decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# Define learning rate schedule\n",
    "warmup_schedule = tfm.optimization.lr_schedule.LinearWarmup(\n",
    "    warmup_learning_rate = 1e-10,\n",
    "    after_warmup_lr_sched = linear_decay,\n",
    "    warmup_steps = warmup_steps\n",
    ")\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = tf.keras.optimizers.experimental.Adam(\n",
    "    learning_rate = warmup_schedule\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BERT and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20/20 [==============================] - 49s 231ms/step - loss: 0.5728 - accuracy: 0.6974 - val_loss: 0.5024 - val_accuracy: 0.7450\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 3s 166ms/step - loss: 0.3082 - accuracy: 0.9388 - val_loss: 0.3753 - val_accuracy: 0.8250\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 3s 165ms/step - loss: 0.2368 - accuracy: 0.9662 - val_loss: 0.3571 - val_accuracy: 0.8400\n",
      "It took 55.392523765563965 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model\n",
    "bert_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Train the model\n",
    "bert_classifier.fit(\n",
    "      x=train_inputs, \n",
    "      y=train_y,\n",
    "      validation_data=(valid_inputs, valid_y),\n",
    "      validation_batch_size=eval_batch_size,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs\n",
    ")\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:HDF5 format does not save weights of `optimizer_experimental.Optimizer`, your optimizer will be recompiled at loading time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:HDF5 format does not save weights of `optimizer_experimental.Optimizer`, your optimizer will be recompiled at loading time.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(bert_classifier, os.path.join('models', 'bert_spam.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 70ms/step - loss: 0.3622 - accuracy: 0.8200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3622386157512665, 0.8199999928474426]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 13.2\n",
    "bert_classifier.evaluate(test_inputs, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manning.tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:51:29) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9a5eda5f6f277a35ee74e53c7ea4300c8d4d4a1e37dd21ebec0e7d7b2134f5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
